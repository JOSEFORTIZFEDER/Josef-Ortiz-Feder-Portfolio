{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPU8SMS+ymAgLuneGzxGxX8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Part 1\n"],"metadata":{"id":"ifL3L0K29lNI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hB4fRcJcEtrF"},"outputs":[],"source":["import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ConvNet(nn.Module):\n","    def __init__(self, mode):\n","        super(ConvNet, self).__init__()\n","\n","        # Define various layers here\n","        self.fc1 = nn.Linear(28*28, 100)\n","        self.fc2 = nn.Linear(100, 100)\n","        self.fc3 = nn.Linear(100, 10)\n","        self.m1_fc1 = nn.Linear(28*28, 20)\n","        self.m1_fc2 = nn.Linear(20, 15)\n","        self.m1_fc3 = nn.Linear(15, 10)\n","        self.m3_fc1 = nn.Linear(28*28, 400)\n","        self.m3_fc2 = nn.Linear(400, 350)\n","        self.m3_fc3 = nn.Linear(350, 250)\n","        self.m3_fc4 = nn.Linear(250, 200)\n","        self.m3_fc5 = nn.Linear(200, 10)\n","\n","        # This will select the forward pass function based on mode for the ConvNet.\n","        # During creation of each ConvNet model, you will assign one of the valid mode.\n","        # This will fix the forward function (and the network graph) for the entire training/testing\n","        if mode == 1:\n","            self.forward = self.model_1\n","        elif mode == 2:\n","            self.forward = self.model_2\n","        elif mode == 3:\n","            self.forward = self.model_3\n","        else:\n","            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n","            exit(0)\n","\n","    # Baseline sample model\n","    def model_0(self, X):\n","        # ======================================================================\n","        # Three fully connected layers with activation\n","\n","        X = torch.flatten(X, start_dim=1)\n","        X = self.fc1(X)\n","        X = F.relu(X)\n","        X = self.fc2(X)\n","        X = F.relu(X)\n","        X = self.fc3(X)\n","        X = torch.sigmoid(X)\n","\n","        return X\n","\n","    # Baseline model. task 1\n","    def model_1(self, X):\n","        # ======================================================================\n","        # Three fully connected layers without activation\n","\n","        X = torch.flatten(X, start_dim=1)\n","        # NotImplementedError\n","        X = self.m1_fc1(X)\n","        X = self.m1_fc2(X)\n","        X = self.m1_fc3(X)\n","        return X\n","\n","\n","    # task 2\n","    def model_2(self, X):\n","        # ======================================================================\n","        # Train with activation (use model 1 from task 1)\n","\n","        X = torch.flatten(X, start_dim=1)\n","        X = self.m1_fc1(X)\n","        X = F.relu(X)\n","        X = self.m1_fc2(X)\n","        X = F.relu(X)\n","        X = self.m1_fc3(X)\n","        X = F.relu(X)\n","        # NotImplementedError\n","\n","        return X\n","\n","\n","    # task 3\n","    def model_3(self, X):\n","        # ======================================================================\n","        # Change number of fully connected layers and number of neurons from model 2 in task 2\n","        self.fc1 = nn.Linear(28*28, 200)\n","        self.fc2 = nn.Linear(100, 100)\n","        self.fc3 = nn.Linear(100, 10)\n","\n","        X = torch.flatten(X, start_dim=1)\n","        X = self.m3_fc1(X)\n","        X = F.relu(X)\n","        X = self.m3_fc2(X)\n","        X = F.relu(X)\n","        X = self.m3_fc3(X)\n","        X = F.relu(X)\n","        X = self.m3_fc4(X)\n","        X = F.relu(X)\n","        X = self.m3_fc5(X)\n","        X = F.relu(X)\n","        # NotImplementedError\n","\n","        return X\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","X = torch.randn(16, 3, 128, 96)\n","print(X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV8_uJhFOeRu","executionInfo":{"status":"ok","timestamp":1701867871465,"user_tz":300,"elapsed":524,"user":{"displayName":"Josef Ortiz","userId":"06946484307351046884"}},"outputId":"de9c1352-d94d-487d-c208-212cf6c2897b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 3, 128, 96])\n"]}]},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.utils.tensorboard import SummaryWriter\n","#from ConvNet import ConvNet\n","import argparse\n","import numpy as np\n","\n","\n","def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n","    '''\n","    Trains the model for an epoch and optimizes it.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    train_loader: dataloader for training samples.\n","    optimizer: optimizer to use for model parameter updates.\n","    criterion: used to compute loss for prediction and target\n","    epoch: Current epoch to train for.\n","    batch_size: Batch size to be used.\n","    '''\n","\n","    # Set model to train mode before each epoch\n","    model.train()\n","\n","    # Empty list to store losses\n","    losses = []\n","    correct = 0\n","\n","    # Iterate over entire training samples (1 epoch)\n","    for batch_idx, batch_sample in enumerate(train_loader):\n","        data, target = batch_sample\n","\n","        # Push data/label to correct device\n","        data, target = data.to(device), target.to(device)\n","\n","        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n","        optimizer.zero_grad()\n","\n","        # Do forward pass for current set of data\n","        output = model(data)\n","\n","        # ======================================================================\n","        # Compute loss based on criterion\n","        # ----------------- YOUR CODE HERE ----------------------\n","        #\n","        # Remove NotImplementedError and assign correct loss function.\n","        #loss = nn.MSELoss(model)\n","        #criterion = nn.MSELoss()\n","        loss = criterion(output, target)\n","\n","\n","        # Computes gradient based on final loss\n","        loss.backward()\n","\n","        # Store loss\n","        losses.append(loss.item())\n","\n","        # Optimize model parameters based on learning rate and gradient\n","        optimizer.step()\n","\n","        # Get predicted index by selecting maximum log-probability\n","        pred = output.argmax(dim=1, keepdim=True)\n","\n","        # ======================================================================\n","        # Count correct predictions overall\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    train_loss = float(np.mean(losses))\n","    train_acc = correct / ((batch_idx+1) * batch_size)\n","    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n","        100. * correct / ((batch_idx+1) * batch_size)))\n","    return train_loss, train_acc\n","\n","\n","\n","def test(model, device, test_loader):\n","    '''\n","    Tests the model.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    test_loader: dataloader for test samples.\n","    '''\n","\n","    # Set model to eval mode to notify all layers.\n","    model.eval()\n","\n","    losses = []\n","    correct = 0\n","\n","    # Set torch.no_grad() to disable gradient computation and backpropagation\n","    with torch.no_grad():\n","        for batch_idx, sample in enumerate(test_loader):\n","            data, target = sample\n","            data, target = data.to(device), target.to(device)\n","\n","\n","            # Predict for data by doing forward pass\n","            output = model(data)\n","\n","            # ======================================================================\n","            # Compute loss based on same criterion as training\n","            loss = F.cross_entropy(output, target, reduction='mean')\n","\n","            # Append loss to overall test loss\n","            losses.append(loss.item())\n","\n","            # Get predicted index by selecting maximum log-probability\n","            pred = output.argmax(dim=1, keepdim=True)\n","\n","            # ======================================================================\n","            # Count correct predictions overall\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss = float(np.mean(losses))\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        test_loss, correct, len(test_loader.dataset), accuracy))\n","\n","    return test_loss, accuracy\n","\n","\n","def run_main(FLAGS):\n","    # Check if cuda is available\n","    use_cuda = torch.cuda.is_available()\n","\n","    # Set proper device based on cuda availability\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    print(\"Torch device selected: \", device)\n","\n","    # Initialize the model and send to device\n","    model = ConvNet(FLAGS.mode).to(device)\n","\n","    # Initialize the criterion for loss computation\n","    # ======================================================================\n","    # Remove NotImplementedError and assign correct loss function.\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    # Initialize optimizer type\n","    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n","\n","    # Create transformations to apply to each data sample\n","    # Can specify variations such as image flip, color flip, random crop, ...\n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","\n","    # Load datasets for training and testing\n","    # Inbuilt datasets available in torchvision (check documentation online)\n","    dataset1 = datasets.MNIST('./data/', train=True, download=True,\n","                       transform=transform)\n","    dataset2 = datasets.MNIST('./data/', train=False,\n","                       transform=transform)\n","    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size,\n","                                shuffle=True, num_workers=4)\n","    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size,\n","                                shuffle=False, num_workers=4)\n","\n","    best_accuracy = 0.0\n","\n","    # Run training for n_epochs specified in config\n","    for epoch in range(1, FLAGS.num_epochs + 1):\n","        print(\"\\nEpoch: \", epoch)\n","        train_loss, train_accuracy = train(model, device, train_loader,\n","                                            optimizer, criterion, epoch, FLAGS.batch_size)\n","        test_loss, test_accuracy = test(model, device, test_loader)\n","\n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","\n","    print(\"accuracy is {:2.2f}\".format(best_accuracy))\n","\n","    print(\"Training and evaluation finished\")\n","\n","\n","if __name__ == '__main__':\n","    # Set parameters for Sparse Autoencoder\n","    parser = argparse.ArgumentParser('CNN Exercise.')\n","    parser.add_argument('--mode',\n","                        type=int, default=2,\n","                        help='Select mode between 1-3.')\n","    parser.add_argument('--learning_rate',\n","                        type=float, default=0.01,\n","                        help='Initial learning rate.')\n","    parser.add_argument('--num_epochs',\n","                        type=int,\n","                        default=20,\n","                        help='Number of epochs to run trainer.')\n","    parser.add_argument('--batch_size',\n","                        type=int, default=10,\n","                        help='Batch size. Must divide evenly into the dataset sizes.')\n","    parser.add_argument('--log_dir',\n","                        type=str,\n","                        default='logs',\n","                        help='Directory to put logging.')\n","\n","    FLAGS = None\n","    FLAGS, unparsed = parser.parse_known_args()\n","\n","    print(\"Mode: \", FLAGS.mode)\n","    print(\"LR: \", FLAGS.learning_rate)\n","    print(\"Batch size: \", FLAGS.batch_size)\n","\n","    run_main(FLAGS)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4s9XyNKcEye0","executionInfo":{"status":"ok","timestamp":1701745088308,"user_tz":300,"elapsed":960332,"user":{"displayName":"Josef Ortiz","userId":"06946484307351046884"}},"outputId":"379e6c99-0bb6-4c7e-808b-82117d416878"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  2\n","LR:  0.01\n","Batch size:  10\n","Torch device selected:  cpu\n","\n","Epoch:  1\n","Train set: Average loss: 1.3481, Accuracy: 28043/60000 (47%)\n","Test set: Average loss: 1.2446, Accuracy: 4882/10000 (49%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.2329, Accuracy: 29534/60000 (49%)\n","Test set: Average loss: 1.2137, Accuracy: 4954/10000 (50%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.2098, Accuracy: 29742/60000 (50%)\n","Test set: Average loss: 1.2073, Accuracy: 4961/10000 (50%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.1963, Accuracy: 29888/60000 (50%)\n","Test set: Average loss: 1.1939, Accuracy: 5004/10000 (50%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.1878, Accuracy: 29994/60000 (50%)\n","Test set: Average loss: 1.1897, Accuracy: 4989/10000 (50%)\n","\n","Epoch:  6\n","Train set: Average loss: 1.1818, Accuracy: 30058/60000 (50%)\n","Test set: Average loss: 1.1893, Accuracy: 4982/10000 (50%)\n","\n","Epoch:  7\n","Train set: Average loss: 1.1772, Accuracy: 30073/60000 (50%)\n","Test set: Average loss: 1.1903, Accuracy: 4987/10000 (50%)\n","\n","Epoch:  8\n","Train set: Average loss: 1.1735, Accuracy: 30129/60000 (50%)\n","Test set: Average loss: 1.1843, Accuracy: 5005/10000 (50%)\n","\n","Epoch:  9\n","Train set: Average loss: 1.1709, Accuracy: 30138/60000 (50%)\n","Test set: Average loss: 1.1863, Accuracy: 4980/10000 (50%)\n","\n","Epoch:  10\n","Train set: Average loss: 1.1683, Accuracy: 30167/60000 (50%)\n","Test set: Average loss: 1.1903, Accuracy: 5014/10000 (50%)\n","\n","Epoch:  11\n","Train set: Average loss: 1.1655, Accuracy: 30179/60000 (50%)\n","Test set: Average loss: 1.1866, Accuracy: 4990/10000 (50%)\n","\n","Epoch:  12\n","Train set: Average loss: 1.1638, Accuracy: 30195/60000 (50%)\n","Test set: Average loss: 1.1888, Accuracy: 4975/10000 (50%)\n","\n","Epoch:  13\n","Train set: Average loss: 1.1623, Accuracy: 30227/60000 (50%)\n","Test set: Average loss: 1.1909, Accuracy: 4975/10000 (50%)\n","\n","Epoch:  14\n","Train set: Average loss: 1.1605, Accuracy: 30240/60000 (50%)\n","Test set: Average loss: 1.1882, Accuracy: 5011/10000 (50%)\n","\n","Epoch:  15\n","Train set: Average loss: 1.1599, Accuracy: 30241/60000 (50%)\n","Test set: Average loss: 1.1862, Accuracy: 4997/10000 (50%)\n","\n","Epoch:  16\n","Train set: Average loss: 1.1580, Accuracy: 30261/60000 (50%)\n","Test set: Average loss: 1.1843, Accuracy: 4994/10000 (50%)\n","\n","Epoch:  17\n","Train set: Average loss: 1.1579, Accuracy: 30266/60000 (50%)\n","Test set: Average loss: 1.1899, Accuracy: 4991/10000 (50%)\n","\n","Epoch:  18\n","Train set: Average loss: 1.1554, Accuracy: 30298/60000 (50%)\n","Test set: Average loss: 1.1883, Accuracy: 5000/10000 (50%)\n","\n","Epoch:  19\n","Train set: Average loss: 1.1550, Accuracy: 30300/60000 (50%)\n","Test set: Average loss: 1.1929, Accuracy: 4996/10000 (50%)\n","\n","Epoch:  20\n","Train set: Average loss: 1.1533, Accuracy: 30320/60000 (51%)\n","Test set: Average loss: 1.1960, Accuracy: 5021/10000 (50%)\n","accuracy is 50.21\n","Training and evaluation finished\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GhSesirC7I6N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Task 1: Model 1 (learning rate = 0.01, epochs=20, batch_size=10)\n","Train loss: 0.2657 | Accuracy: 92%\n","Test loss: 0.2900 | Accuracy: 92%\n","\n","Task 2:Model 2 (learning rate = 0.01, epochs=20, batch_size=10) Train loss: 0.0748 | Accuracy: 98% Test loss: 0.1500 | Accuracy: 96%\n","\n","Task 3:Model 3 (learning rate = 0.01, epochs=20, batch_size=10) Train loss: 0.2287 | Accuracy: 90% Test loss: 0.2930 | Accuracy: 89%\n","\n","Observations: It seems as though model 2 did the best is their respective iterations. However, all three models did very well as they all had accuracies around 90% with losses less than 1. It is interesting to see how each model improves throughout the epochs. For example, model 3 started at an accuracy of 70% and 86% for training and test respectively and ended up improving until they hit 90% and 89% at the 8th epoch and stayed the same throughout the rest of the runtime.\n","All three models took longer than I had assumed in their runtime. Model 1 ran 15 minutes, model 2 ran for 16 minutes, and model 3 ran for 27 minutes although that could just be google colab's runtime limitations. It makes sense though that the runtimes would increase over the models because the complexity increases as well."],"metadata":{"id":"_erat6uhiTlu"}},{"cell_type":"markdown","source":["**Part 2**"],"metadata":{"id":"H2pA82Sr7MTk"}},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ConvNet(nn.Module):\n","    def __init__(self, mode):\n","        super(ConvNet, self).__init__()\n","\n","        # Define various layers here, such as in the tutorial example\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n","        self.m1_conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)\n","        self.m2_conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3)\n","        self.m2_conv2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3)\n","        #self.conv2 = Make conv layer like above\n","        #self.conv3 = Make conv layer like above\n","\n","        self.fc1_model1 = nn.Linear(360, 100)  # This is first fully connected layer for step 1.\n","        self.fc1_model2 = nn.Linear(1440, 100) # This is first fully connected layer for step 2.\n","        self.fc1_model3 = nn.Linear(640, 100)  # This is first fully connected layer for step 3\n","\n","        self.fc2 = nn.Linear(100, 10)       # This is 2nd fully connected layer for all models.\n","\n","        self.fc_model0 = nn.Linear(2250, 100)   # This is for example model.\n","\n","\n","        # This will select the forward pass function based on mode for the ConvNet.\n","        # Based on the question, you have 3 modes available for step 1 to 3.\n","        # During creation of each ConvNet model, you will assign one of the valid mode.\n","        # This will fix the forward function (and the network graph) for the entire training/testing\n","        if mode == 1:\n","            self.forward = self.model_1\n","        elif mode == 2:\n","            self.forward = self.model_2\n","        elif mode == 3:\n","            self.forward = self.model_3\n","        elif mode == 0:\n","            self.forward = self.model_0\n","        else:\n","            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n","            exit(0)\n","\n","\n","    # Example model. Modify this for step 1-3\n","    def model_0(self, X):\n","        # ======================================================================\n","\n","        X = F.relu(self.conv1(X))\n","        #print(X.shape)\n","        X = F.max_pool2d(X, kernel_size=2)\n","        #print(X.shape)\n","\n","        X = torch.flatten(X, start_dim=1)\n","        #print(X.shape)\n","\n","        X = F.relu(self.fc_model0(X))\n","        X = self.fc2(X)\n","\n","        return X\n","\n","\n","    # Simple CNN. step 1\n","    def model_1(self, X):\n","        # ======================================================================\n","\n","        # Complete this part as model_0, add one more conv2d layer\n","        # with relu activation followed by maxpool layer.\n","        X = F.relu(self.conv1(X))\n","        X = F.max_pool2d(X, kernel_size=2)\n","        X = F.relu(self.m1_conv2(X))\n","        X = F.max_pool2d(X,kernel_size=2)\n","\n","        X = torch.flatten(X, start_dim=1)\n","\n","        #X = F.relu(self.fc1_model1(X))\n","        X = self.fc1_model1(X)\n","        X = self.fc2(X)\n","\n","        return X\n","\n","\n","    # Increase filters. step 2\n","    def model_2(self, X):\n","        # ======================================================================\n","\n","        # Complete this part as model_1. Modify in/out channels for conv2d layers.\n","        X = F.relu(self.m2_conv1(X))\n","        X = F.max_pool2d(X, kernel_size=2)\n","        X = F.relu(self.m2_conv2(X))\n","        X = F.max_pool2d(X, kernel_size=2)\n","\n","        X = torch.flatten(X, start_dim=1)\n","\n","        #X = F.relu(self.fc1_model2(X))\n","        X = self.fc1_model2(X)\n","        X = self.fc2(X)\n","\n","        return X\n","\n","\n","    # Large CNN. step 3\n","    def model_3(self, X):\n","        # ======================================================================\n","\n","        # Complete this part as model_2, add one more conv2d layer\n","        # with relu activation. Do not add maxpool after this new conv2d layer.\n","\n","        X = torch.flatten(X, start_dim=1)\n","\n","        X = F.relu(self.fc1_model3(X))\n","        X = self.fc2(X)\n","\n","        return X\n","\n","\n","\n","\n"],"metadata":{"id":"d8U_GSDb7J5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.utils.tensorboard import SummaryWriter\n","#from ConvNet import ConvNet\n","import argparse\n","import numpy as np\n","\n","def train(model, device, train_loader, optimizer, criterion, epoch, batch_size):\n","    '''\n","    Trains the model for an epoch and optimizes it.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    train_loader: dataloader for training samples.\n","    optimizer: optimizer to use for model parameter updates.\n","    criterion: used to compute loss for prediction and target\n","    epoch: Current epoch to train for.\n","    batch_size: Batch size to be used.\n","    '''\n","\n","    # Set model to train mode before each epoch\n","    model.train()\n","\n","    # Empty list to store losses\n","    losses = []\n","    correct = 0\n","\n","    # Iterate over entire training samples (1 epoch)\n","    for batch_idx, batch_sample in enumerate(train_loader):\n","        data, target = batch_sample\n","\n","        # Push data/label to correct device\n","        data, target = data.to(device), target.to(device)\n","        # print(data.shape)\n","        # print(target.shape)\n","        # exit()\n","\n","        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n","        optimizer.zero_grad()\n","\n","        # Do forward pass for current set of data\n","        output = model(data)\n","\n","        # ======================================================================\n","        # Compute loss based on criterion\n","        loss = criterion(output, target)\n","\n","\n","        # Computes gradient based on final loss\n","        loss.backward()\n","\n","        # Store loss\n","        losses.append(loss.item())\n","\n","        # Optimize model parameters based on learning rate and gradient\n","        optimizer.step()\n","\n","        # Get predicted index by selecting maximum log-probability\n","        pred = output.argmax(dim=1, keepdim=True)\n","\n","        # ======================================================================\n","        # Count correct predictions overall\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    train_loss = float(np.mean(losses))\n","    train_acc = correct / ((batch_idx+1) * batch_size)\n","    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n","        100. * correct / ((batch_idx+1) * batch_size)))\n","    return train_loss, train_acc\n","\n","\n","\n","def test(model, device, test_loader):\n","    '''\n","    Tests the model.\n","    model: The model to train. Should already be in correct device.\n","    device: 'cuda' or 'cpu'.\n","    test_loader: dataloader for test samples.\n","    '''\n","\n","    # Set model to eval mode to notify all layers.\n","    model.eval()\n","\n","    losses = []\n","    correct = 0\n","\n","    # Set torch.no_grad() to disable gradient computation and backpropagation\n","    with torch.no_grad():\n","        for batch_idx, sample in enumerate(test_loader):\n","            data, target = sample\n","            data, target = data.to(device), target.to(device)\n","\n","\n","            # Predict for data by doing forward pass\n","            output = model(data)\n","\n","            # ======================================================================\n","            # Compute loss based on same criterion as training\n","            loss = F.cross_entropy(output, target, reduction='mean')\n","\n","            # Append loss to overall test loss\n","            losses.append(loss.item())\n","\n","            # Get predicted index by selecting maximum log-probability\n","            pred = output.argmax(dim=1, keepdim=True)\n","\n","            # ======================================================================\n","            # Count correct predictions overall\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss = float(np.mean(losses))\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n","        test_loss, correct, len(test_loader.dataset), accuracy))\n","\n","    return test_loss, accuracy\n","\n","\n","def run_main(FLAGS):\n","    # Check if cuda is available\n","    use_cuda = torch.cuda.is_available()\n","\n","    # Set proper device based on cuda availability\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    print(\"Torch device selected: \", device)\n","\n","    # Initialize the model and send to device\n","    model = ConvNet(FLAGS.mode).to(device)\n","    # print(model)\n","    # exit()\n","\n","    # Initialize the criterion for loss computation\n","    criterion = nn.CrossEntropyLoss(reduction='mean')\n","\n","    # Initialize optimizer type\n","    optimizer = optim.SGD(model.parameters(), lr=FLAGS.learning_rate, weight_decay=1e-7)\n","\n","    # Create transformations to apply to each data sample\n","    # Can specify variations such as image flip, color flip, random crop, ...\n","    #transform=transforms.Compose([\n","    #    transforms.ToTensor(),\n","    #    transforms.Normalize((0.1307,), (0.3081,))\n","    #    ])\n","\n","    transform = transforms.Compose(\n","                    [transforms.ToTensor(),\n","                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    # Load datasets for training and testing\n","    # Inbuilt datasets available in torchvision (check documentation online)\n","    dataset1 = datasets.CIFAR10('./data/', train=True, download=True,\n","                       transform=transform)\n","    dataset2 = datasets.CIFAR10('./data/', train=False,\n","                       transform=transform)\n","    train_loader = DataLoader(dataset1, batch_size = FLAGS.batch_size,\n","                                shuffle=True, num_workers=4)\n","    test_loader = DataLoader(dataset2, batch_size = FLAGS.batch_size,\n","                                shuffle=False, num_workers=4)\n","\n","    best_accuracy = 0.0\n","\n","    # Run training for n_epochs specified in config\n","    for epoch in range(1, FLAGS.num_epochs + 1):\n","        print(\"\\nEpoch: \", epoch)\n","        train_loss, train_accuracy = train(model, device, train_loader,\n","                                            optimizer, criterion, epoch, FLAGS.batch_size)\n","        test_loss, test_accuracy = test(model, device, test_loader)\n","\n","        if test_accuracy > best_accuracy:\n","            best_accuracy = test_accuracy\n","\n","    print(\"accuracy is {:2.2f}\".format(best_accuracy))\n","\n","    print(\"Training and evaluation finished\")\n","\n","\n","if __name__ == '__main__':\n","    # Set parameters for Sparse Autoencoder\n","    parser = argparse.ArgumentParser('CNN Exercise.')\n","    parser.add_argument('--mode',\n","                        type=int, default=1,\n","                        help='Select mode between 1-3.')\n","    parser.add_argument('--learning_rate',\n","                        type=float, default=0.01,\n","                        help='Initial learning rate.')\n","    parser.add_argument('--num_epochs',\n","                        type=int,\n","                        default=10,\n","                        help='Number of epochs to run trainer.')\n","    parser.add_argument('--batch_size',\n","                        type=int, default=32,\n","                        help='Batch size. Must divide evenly into the dataset sizes.')\n","    parser.add_argument('--log_dir',\n","                        type=str,\n","                        default='logs',\n","                        help='Directory to put logging.')\n","\n","    FLAGS = None\n","    FLAGS, unparsed = parser.parse_known_args()\n","\n","    print(\"Mode: \", FLAGS.mode)\n","    print(\"LR: \", FLAGS.learning_rate)\n","    print(\"Batch size: \", FLAGS.batch_size)\n","\n","    run_main(FLAGS)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sla5op1D7Rhm","executionInfo":{"status":"ok","timestamp":1701745881101,"user_tz":300,"elapsed":424567,"user":{"displayName":"Josef Ortiz","userId":"06946484307351046884"}},"outputId":"f292d1e0-baa7-412d-9856-18aa57de12a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mode:  1\n","LR:  0.01\n","Batch size:  32\n","Torch device selected:  cpu\n","Files already downloaded and verified\n","\n","Epoch:  1\n","Train set: Average loss: 2.0462, Accuracy: 12727/50016 (25%)\n","Test set: Average loss: 1.8329, Accuracy: 3560/10000 (36%)\n","\n","Epoch:  2\n","Train set: Average loss: 1.6998, Accuracy: 19801/50016 (40%)\n","Test set: Average loss: 1.5508, Accuracy: 4464/10000 (45%)\n","\n","Epoch:  3\n","Train set: Average loss: 1.4803, Accuracy: 23658/50016 (47%)\n","Test set: Average loss: 1.4661, Accuracy: 4802/10000 (48%)\n","\n","Epoch:  4\n","Train set: Average loss: 1.4000, Accuracy: 25197/50016 (50%)\n","Test set: Average loss: 1.3737, Accuracy: 5195/10000 (52%)\n","\n","Epoch:  5\n","Train set: Average loss: 1.3480, Accuracy: 26202/50016 (52%)\n","Test set: Average loss: 1.3736, Accuracy: 5204/10000 (52%)\n","\n","Epoch:  6\n","Train set: Average loss: 1.3037, Accuracy: 26922/50016 (54%)\n","Test set: Average loss: 1.3252, Accuracy: 5331/10000 (53%)\n","\n","Epoch:  7\n","Train set: Average loss: 1.2690, Accuracy: 27585/50016 (55%)\n","Test set: Average loss: 1.2679, Accuracy: 5568/10000 (56%)\n","\n","Epoch:  8\n","Train set: Average loss: 1.2390, Accuracy: 28206/50016 (56%)\n","Test set: Average loss: 1.2489, Accuracy: 5596/10000 (56%)\n","\n","Epoch:  9\n","Train set: Average loss: 1.2126, Accuracy: 28569/50016 (57%)\n","Test set: Average loss: 1.2192, Accuracy: 5702/10000 (57%)\n","\n","Epoch:  10\n","Train set: Average loss: 1.1881, Accuracy: 29043/50016 (58%)\n","Test set: Average loss: 1.1910, Accuracy: 5781/10000 (58%)\n","accuracy is 57.81\n","Training and evaluation finished\n"]}]},{"cell_type":"markdown","source":["Task 4:\n","\n","For learning rate 0.5: Training Loss: 1.7937 | Training Accuracy: 35% Testing Loss: 1.9294 | Testing Accuracy: 30%\n","\n","For learning rate 0.1: Training Loss: 1.1643 | Training Accuracy: 60% Testing Loss: 1.2428 | Testing Accuracy: 56%\n","\n","\n","For learning rate 0.01: Training Loss: 1.1494 | Training Accuracy: 60% Testing Loss: 1.1834 | Testing Accuracy: 58%\n","\n","Observations: The learning rate did seem to have an impact in how the models ran. There is a pattern of the lower the learning rate, the better the model does. I could also see this happening during the runtime with each epoch. I noticed that for the learning rate of 0.5, the accuracies seemed to oscillate between being overfit and underfit and couldn't get that far in improving them while for the learning rate of 0.1, the accuracies started off differnt in the first epoch but immediately came together on the second and was consistent throughout.\n","Task 5:\n","\n","For 10 epochs:\n","Training Loss: 0.7996 | Training Accuracy: 72%\n","Testing Loss: 1.0160 | Testing Accuracy: 65%\n","\n","For 20 epochs:\n","Training Loss: 0.7217 | Training Accuracy: 75%\n","Testing Loss: 1.0194 | Testing Accuracy: 67%\n","\n","For 30 epochs:\n","Training Loss: 0.6866 | Training Accuracy: 76%\n","Testing Loss: 1.0808 | Testing Accuracy: 66%\n","\n","Observations: I was expecting for the higher epochs to output a better result but that does not seem to be the case in this problem in terms of accuracy. For all three iterations, the output was relatively the same except for the loss where I did notice that the losses did seem to decrease when given higher epochs to run."],"metadata":{"id":"j88Bl1rx9i9w"}}]}